---
title: "IST687 Final Project"
output: pdf_document
---
###Here is the url for the dataset we're using in our project:
https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95/data

### Part 1: Formatting Data with workable column names: 
```{r}
#importing in the data:
#import dataset>browse>import
```

```{r}
#naming the dataset:
MVCC_OG <- Motor_Vehicle_Collisions_Crashes

#checking the number of rows: should be 443,045
nrow(MVCC_OG)

#Looking at the col-names:
column_names <- colnames(MVCC_OG)

#Replacing white space with "_"
library(stringi)
column_names_new <- stri_replace_all_charclass(column_names,"\\p{WHITE_SPACE}", "_")
MVCC_toclean <- MVCC_OG

#Renaming column names with cleaned up column names
colnames(MVCC_toclean) <-column_names_new

#New data frame with cleaned up column names: MVCC_toclean
```

### Part 2: Cleaning whitespace in all other entries:
```{r}
#Getting rid of white space in Staten Island entry in "BOROUGH":
MVCC_toclean$BOROUGH <- stri_replace_all_charclass(MVCC_toclean$BOROUGH,"\\p{WHITE_SPACE}", "_")

#Getting rid of white space in all entries  in "ON_STREET_NAME":
MVCC_toclean$ON_STREET_NAME <- stri_replace_all_charclass(MVCC_toclean$ON_STREET_NAME,"\\p{WHITE_SPACE}", "_")

#Getting rid of white space in all entries  in "CROSS_STREET_NAME":
MVCC_toclean$CROSS_STREET_NAME <- stri_replace_all_charclass(MVCC_toclean$CROSS_STREET_NAME,"\\p{WHITE_SPACE}", "_")

#Getting rid of white space in all entries  in "OFF_STREET_NAME":
MVCC_toclean$OFF_STREET_NAME <- stri_replace_all_charclass(MVCC_toclean$OFF_STREET_NAME,"\\p{WHITE_SPACE}", "_")

#Getting rid of white space in all entries  in "CONTRIBUTING_FACTOR_VEHICLE_1":
MVCC_toclean$CONTRIBUTING_FACTOR_VEHICLE_1 <- stri_replace_all_charclass(MVCC_toclean$CONTRIBUTING_FACTOR_VEHICLE_1,"\\p{WHITE_SPACE}", "_")

#Getting rid of white space in all entries  in "CONTRIBUTING_FACTOR_VEHICLE_2":
MVCC_toclean$CONTRIBUTING_FACTOR_VEHICLE_2 <- stri_replace_all_charclass(MVCC_toclean$CONTRIBUTING_FACTOR_VEHICLE_2,"\\p{WHITE_SPACE}", "_")

#Getting rid of white space in all entries  in "CONTRIBUTING_FACTOR_VEHICLE_3":
MVCC_toclean$CONTRIBUTING_FACTOR_VEHICLE_3 <- stri_replace_all_charclass(MVCC_toclean$CONTRIBUTING_FACTOR_VEHICLE_3,"\\p{WHITE_SPACE}", "_")

#Getting rid of white space in all entries  in "CONTRIBUTING_FACTOR_VEHICLE_4":
MVCC_toclean$CONTRIBUTING_FACTOR_VEHICLE_4 <- stri_replace_all_charclass(MVCC_toclean$CONTRIBUTING_FACTOR_VEHICLE_4,"\\p{WHITE_SPACE}", "_")

#Getting rid of white space in all entries  in "CONTRIBUTING_FACTOR_VEHICLE_5":
MVCC_toclean$CONTRIBUTING_FACTOR_VEHICLE_5 <- stri_replace_all_charclass(MVCC_toclean$CONTRIBUTING_FACTOR_VEHICLE_5,"\\p{WHITE_SPACE}", "_")

#Getting rid of white space in all entries  in "VEHICLE_TYPE_CODE_1":
MVCC_toclean$VEHICLE_TYPE_CODE_1<- stri_replace_all_charclass(MVCC_toclean$VEHICLE_TYPE_CODE_1,"\\p{WHITE_SPACE}", "_")

#Getting rid of white space in all entries  in "VEHICLE_TYPE_CODE_2":
MVCC_toclean$VEHICLE_TYPE_CODE_2 <- stri_replace_all_charclass(MVCC_toclean$VEHICLE_TYPE_CODE_2,"\\p{WHITE_SPACE}", "_")

#Getting rid of white space in all entries  in "VEHICLE_TYPE_CODE_3":
MVCC_toclean$VEHICLE_TYPE_CODE_3 <- stri_replace_all_charclass(MVCC_toclean$VEHICLE_TYPE_CODE_3,"\\p{WHITE_SPACE}", "_")

#Getting rid of white space in all entries  in "VEHICLE_TYPE_CODE_4":
MVCC_toclean$VEHICLE_TYPE_CODE_4 <- stri_replace_all_charclass(MVCC_toclean$VEHICLE_TYPE_CODE_4,"\\p{WHITE_SPACE}", "_")

#Getting rid of white space in all entries  in "VEHICLE_TYPE_CODE_5":
MVCC_toclean$VEHICLE_TYPE_CODE_5 <- stri_replace_all_charclass(MVCC_toclean$VEHICLE_TYPE_CODE_5,"\\p{WHITE_SPACE}", "_")

#Examining Structure to see what else needs to be cleaned
#str(MVCC_toclean)
```

### Part 3 Cleaning "CRASH_TIME" column:
```{r}

library(tidyverse)
library(dplyr)

#Observed "POSIXct" data-type that automatically inputs in "1899-12-31" before the time.
MVCC_toclean2 <- MVCC_toclean
#converting from POSIXct to as.character:
MVCC_toclean2$CRASH_TIME <- as.character(MVCC_toclean2$CRASH_TIME)
#str(MVCC_toclean$CRASH_TIME)

#Replacing whitespace with "a":
MVCC_toclean2$CRASH_TIME <- stri_replace_all_charclass(MVCC_toclean2$CRASH_TIME,"\\p{WHITE_SPACE}", "a")

#Using gsub() to get rid of everything before "a"
MVCC_toclean2$CRASH_TIME <-gsub(".*a","",MVCC_toclean2$CRASH_TIME)

#checking to see if it worked:
#head(MVCC_toclean2$CRASH_TIME)

#creating HOUR, MINUTE, SECOND columns:
MVCC_toclean3 <- separate(MVCC_toclean2,CRASH_TIME,into= c('HOUR','MINUTE','SECOND'))

#setting HOUR, MINUTE, and SECOND as numeric variables:
MVCC_toclean3$HOUR <- as.numeric(MVCC_toclean3$HOUR)
MVCC_toclean3$MINUTE <- as.numeric(MVCC_toclean3$MINUTE)
MVCC_toclean3$SECOND <- as.numeric(MVCC_toclean3$SECOND)

#checking to see if it worked:
#head(MVCC_toclean3)
```

### Part 4: Cleaning "CRASH_DATE" column:
```{r}
#Separating CRASH_DATE into YEAR, MONTH, AND DAY:
MVCC_toclean4 <- separate(MVCC_toclean3,CRASH_DATE, into = c('YEAR', 'MONTH', 'DAY') )

#setting HOUR, MINUTE, and SECOND as numeric variables:
MVCC_toclean4$YEAR <- as.numeric(MVCC_toclean4$YEAR)
MVCC_toclean4$MONTH <- as.numeric(MVCC_toclean4$MONTH)
MVCC_toclean4$DAY <- as.numeric(MVCC_toclean4$DAY)

#head(MVCC_toclean4)
```

### Part 5: Cleaning Longitude and Latitude
```{r}
#creating a new version so that our past work doesn't get messed up:
testing <- MVCC_toclean4

#What is the min value of Latitude?
min(testing$LATITUDE,na.rm=TRUE)

#Our data latitude minimum should not be 0, this means that we need to investigate this further.

#Examining where all the O for latitude values are:
LATITUDE_ZERO <- testing%>%
  filter(LATITUDE == 0)
#LATITUDE_ZERO

#Observed: there are 589 rows were latitude, longitude of (0,0) 
```

### Part 5A: Where we have the Borough but not the longitude and latitude coordinates:
```{r}
#Found Latitude and Longitude coordinates for each borough via Google Search:
#Brooklyn: (40.6782,-73.9442)
#Manhattan:(40.7831,-73.9712)
#Queens:(40.7282,-73.794)
#Bronx:(40.8448,-73.8648)
#Staten_Island: (40.5795,-74.1502)


#Using known latitude of each borough to fill in the 0 values:
testing1 <- testing %>%
    mutate(LATITUDE = ifelse(LATITUDE==0,case_when(BOROUGH=='BROOKLYN'~40.6782,
                                                 BOROUGH=='MANHATTAN'~40.7831,
                                                 BOROUGH=='QUEENS'~40.7282,
                                                 BOROUGH=='BRONX'~40.8448,
                                                 BOROUGH=='STATEN_ISLAND'~40.5795),LATITUDE))
#testing1                   

#checking to see if it  worked:
check1 <- testing1%>%
  filter(LATITUDE == 0)
#check1

#Using known longitude of each borough to fill in the 0 values:
testing2 <- testing1 %>%
    mutate(LONGITUDE = ifelse(LONGITUDE==0,case_when(BOROUGH=='BROOKLYN'~ -73.9442,
                                              BOROUGH == "MANHATTAN" ~ -73.9712,
                                              BOROUGH == "QUEENS" ~ -73.7949,
                                              BOROUGH == "BRONX" ~ -73.8648,
                                              BOROUGH == "STATEN_ISLAND" ~ -74.1502),LONGITUDE))
#testing2 

#checking to see if it  worked:
check2 <- testing2%>%
  filter(LONGITUDE == 0)
#check2
```   

```{r}
#What is the max and min latitude now?
min(testing2$LATITUDE,na.rm=TRUE)
max(testing2$LATITUDE,na.rm=TRUE)

# This is normal.
```
```{r}
#What is the max and min longitude now?
max(testing2$LONGITUDE,na.rm=TRUE)
min(testing2$LONGITUDE,na.rm=TRUE)

# !!!  This is  extreme.
```

### Part 5B: Cleaning outliers of longitude:
```{r}
#Examininglower end outlier longitude
LONGITUDE_EXTREME<- testing2%>%
  filter(LONGITUDE < -75)
#LONGITUDE_EXTREME

#longitude = -201.2371 even though the 'on street name' is queensboro-bridge_upper_roadway which is in queens. Error in data collection.

#using the cross street we see that the values are "queensborough bridge", using google maps we can find that this is in Queens
#Cleaning so that the lower end extreme values are set to the long of queens
testing3 <- testing2 %>%
    mutate(LONGITUDE = ifelse(LONGITUDE < -75 , -73.7949,LONGITUDE))
#testing3 

#Check
#min(testing3$LONGITUDE,na.rm=TRUE)
#this means that we have cleaned up the extreme since the min longitude value is no longer -201.
```

```{r}
#Examining upper end outlier of longitude
LONGITUDE_EXTREME2<- testing3%>%
  filter(LONGITUDE > -72)
#LONGITUDE_EXTREME2

#using google maps to look up the on street value of "nassau expressway"
#Cleaning so that the upper end extreme values are set to the long of queens
testing4 <- testing3 %>%
    mutate(LONGITUDE = ifelse(LONGITUDE> -72, -73.7949,LONGITUDE))
#testing4
#Check
#max(testing4$LONGITUDE,na.rm=TRUE)
```

```{r}
#Double checking all of longitude and latitude values:
summary(testing4$LONGITUDE,na.rm=TRUE)
summary(testing4$LATITUDE,na.rm=TRUE)

#now all the min and max values are in the  reasonable range. Our min and max values are no longer outliers.
```

### Part 6 Using longitude and latitude to fill in for NA values in Borough and Zip_Code: 
```{r}
testing5 <- testing4
#head(testing5)

#First count the number of NA values of latitude so we can have base number:
sum(is.na(testing5$BOROUGH)) #156,791 

#Staten Island subset
staten <- testing5%>%
  filter(BOROUGH == "STATEN_ISLAND")

staten_lo_min <- min(staten$LONGITUDE,na.rm=TRUE)
staten_lo_max <- max(staten$LONGITUDE,na.rm=TRUE)
staten_lat_min <-min(staten$LATITUDE,na.rm=TRUE)
staten_lat_max <-max(staten$LATITUDE,na.rm=TRUE)

testing6 <- testing5 %>%
    mutate(BOROUGH = ifelse( is.na(BOROUGH),ifelse(LATITUDE>= staten_lat_min & LATITUDE<=staten_lat_max & 
                            LONGITUDE>= staten_lo_min & LONGITUDE<= staten_lo_max, 'STATEN_ISLAND',BOROUGH),BOROUGH))

sum(is.na(testing6$BOROUGH)) #Number of NA values in the Borough column is now 48,855

#Manhattan subset
manhattan <- testing5%>%
  filter(BOROUGH == "MANHATTAN")

manhattan_lo_min <- min(manhattan$LONGITUDE,na.rm=TRUE)
manhattan_lo_max <- max(manhattan$LONGITUDE,na.rm=TRUE)
manhattan_lat_min <- min(manhattan$LATITUDE,na.rm=TRUE)
manhattan_lat_max <- max(manhattan$LATITUDE,na.rm=TRUE)

testing7 <- testing6 %>%
    mutate(BOROUGH = ifelse( is.na(BOROUGH),ifelse(LATITUDE>=manhattan_lat_min & LATITUDE<=manhattan_lat_max & 
                            LONGITUDE>=manhattan_lo_min& LONGITUDE<=manhattan_lo_max, 'MANHATTAN',BOROUGH),BOROUGH))

sum(is.na(testing7$BOROUGH)) #Number of NA values in the Borough column is now 26,929

#BRONX subset
bronx <- testing5%>%
  filter(BOROUGH == "BRONX")

bronx_lo_min <- min(bronx$LONGITUDE,na.rm=TRUE)
bronx_lo_max <- max(bronx$LONGITUDE,na.rm=TRUE)
bronx_lat_min <- min(bronx$LATITUDE,na.rm=TRUE)
bronx_lat_max <- max(bronx$LATITUDE,na.rm=TRUE)

testing8 <- testing7 %>%
    mutate(BOROUGH = ifelse( is.na(BOROUGH),ifelse(LATITUDE>=bronx_lat_min & LATITUDE<=bronx_lat_max & 
                            LONGITUDE>bronx_lo_min& LONGITUDE<=bronx_lo_max, 'BRONX',BOROUGH),BOROUGH))

sum(is.na(testing8$BOROUGH)) #Number of NA values in the Borough column is now 26,113

#BROOKLYN subset
brooklyn <- testing5%>%
  filter(BOROUGH == "BROOKLYN")

brooklyn_lo_min <- min(brooklyn$LONGITUDE,na.rm=TRUE)
brooklyn_lo_max <- max(brooklyn$LONGITUDE,na.rm=TRUE)
brooklyn_lat_min <- min(brooklyn$LATITUDE,na.rm=TRUE)
brooklyn_lat_max <- max(brooklyn$LATITUDE,na.rm=TRUE)

testing9 <- testing8 %>%
    mutate(BOROUGH = ifelse( is.na(BOROUGH),ifelse(LATITUDE>=brooklyn_lat_min & LATITUDE<=brooklyn_lat_max & 
                            LONGITUDE>=brooklyn_lo_min & LONGITUDE<=brooklyn_lo_max, 'BROOKLYN',BOROUGH),BOROUGH))

sum(is.na(testing9$BOROUGH)) #Number of NA in the Borough column is still 26,113

#QUEENS subset
queens <- testing5%>%
  filter(BOROUGH == "QUEENS")

queens_lo_min <- min(queens$LONGITUDE,na.rm=TRUE)
queens_lo_max <- max(queens$LONGITUDE,na.rm=TRUE)
queens_lat_min <- min(queens$LATITUDE,na.rm=TRUE)
queens_lat_max <- max(queens$LATITUDE,na.rm=TRUE)

testing10 <- testing9 %>%
    mutate(BOROUGH = ifelse( is.na(BOROUGH),ifelse(LATITUDE>=queens_lat_min & LATITUDE<=queens_lat_max & 
                            LONGITUDE>=queens_lo_min & LONGITUDE<=queens_lo_max, 'QUEENS',BOROUGH),BOROUGH))

sum(is.na(testing10$BOROUGH)) #Number of NA in Borough column is now 24,275

#how many NA values did we clean?
#156791- 24275  #we have cleaned up 132,516 NA values from the Borough Column
```

### Part 7 Using Borough name to fill in Zip_Code:

```{r}
#use mean of Zip_Codes by Borough to fill in for missing Zipcodes:

staten_mean <- round(mean(testing10$ZIP_CODE[testing$BOROUGH=="STATEN_ISLAND"],na.rm=TRUE),0)
manhattan_mean <- round(mean(testing10$ZIP_CODE[testing$BOROUGH=="MANHATTAN"],na.rm=TRUE),0)
bronx_mean <- round(mean(testing10$ZIP_CODE[testing$BOROUGH=="BRONX"],na.rm=TRUE),0)
brooklyn_mean <- round(mean(testing10$ZIP_CODE[testing$BOROUGH=="BROOKLYN"],na.rm=TRUE),0)
queens_mean <- round(mean(testing10$ZIP_CODE[testing$BOROUGH=="QUEENS"],na.rm=TRUE),0)

#filling:

testing11 <- testing10 %>%
    mutate(ZIP_CODE = case_when(BOROUGH=="STATEN_ISLAND"~ staten_mean,
                                  BOROUGH == "MANHATTAN" ~ manhattan_mean,
                                  BOROUGH == "BRONX" ~ bronx_mean,
                                  BOROUGH == "QUEENS" ~ queens_mean,
                                  BOROUGH == "BROOKLYN" ~ brooklyn_mean),ZIP_CODE)
```

### Part 8: Dropping last na values where we don't have any info to filling in:
```{r}
#Now lets look at our cleaned up dataset and where the NA values are
last_na <- testing11 %>%
  filter(is.na(testing11$BOROUGH))
#last_na

#There are 24,275 rows where there are NA values for Borough, ZipCode, Latitutde and Longitutde. Without a good predictor, there is no way for us to fill in these values.

#Checking what percentage this is of our dataset:
24275/433045 

#This is only 5% of our  data so we will drop these rows
testing12 <- testing11[!is.na(testing11$BOROUGH),]
#testing12

#checking new number of observations:
443045-24275

#the correct number of rows was dropped
```


### Part 9:Examining pedestrians, cyclists, and motorists + creating final cleaned dataset
```{r}
#As a team we have decided to drop the following columns:
df <- subset(testing12, select = -c(CROSS_STREET_NAME,OFF_STREET_NAME, CONTRIBUTING_FACTOR_VEHICLE_2,CONTRIBUTING_FACTOR_VEHICLE_3,CONTRIBUTING_FACTOR_VEHICLE_4,CONTRIBUTING_FACTOR_VEHICLE_5,VEHICLE_TYPE_CODE_3,VEHICLE_TYPE_CODE_4,VEHICLE_TYPE_CODE_5,COLLISION_ID))

df<- df[!is.na(df$LATITUDE),]
#nrow(df[(is.na(df$LATITUDE)),])
df2<- df[!is.na(df$LONGITUDE),]
#nrow(df2[(is.na(df2$LATITUDE)),])

#head(df)
df_test1 <- df2
#Renaming columns so it's easier to code: 
colnames(df_test1)[13] <-"totalI"
colnames(df_test1)[14] <-"totalK"
colnames(df_test1)[15] <-"pedI"
colnames(df_test1)[16] <-"pedK"
colnames(df_test1)[17] <-"cylI"
colnames(df_test1)[18] <-"cylK"
colnames(df_test1)[19] <-"motI"
colnames(df_test1)[20] <-"motK"

#Checking for NA values in ped, cyc, and mot columns
df_test1$totalI[is.na(df_test1$totalI)]
df_test1$totalK[is.na(df_test1$totalK)]
df_test1$pedI[is.na(df_test1$pedI)]
df_test1$pedK[is.na(df_test1$pedK)]
df_test1$cylI[is.na(df_test1$cylI)]
df_test1$cylK[is.na(df_test1$cylK)]
df_test1$motI[is.na(df_test1$motI)]
df_test1$motK[is.na(df_test1$motK)]

#Found NA values in totalI and totalK.
#Replacing totalI NA values with sum of pedI, cylI, and motI
df_test1$totalI <- replace_na(df_test1$pedI + 
                                df_test1$cylI +
                                df_test1$motI)

#Replacing totalI NA values with sum of pedI, cylI, and motI
df_test1$totalK <- replace_na(df_test1$pedK + 
                                df_test1$cylK +
                                df_test1$motK)

#df_test1 

#looks at unique values:
#unique(df_test1$VEHICLE_TYPE_CODE_1)

#making new columns so that we can look at Pedestrians, Cylists  and motorists easily: 
df_test1$Pedestrian <- df_test1$pedI+ df_test1$pedK
df_test1$Cyclist  <-df_test1$cylI + df_test1$cylK
df_test1$Motorist <-df_test1$motI +  df_test1$motK
df_test1$Total <-df_test1$totalI + df_test1$totalK

#df_test1 will  be our finalized cleaned dataset name
```




### Part 10: Data Exploration: 
#Q: What are the trends of incidents per day for each month?
```{r}
#Get number of incidents per day
dfNumIncidentsPerDay <- df_test1 %>%
  group_by(YEAR, MONTH, DAY) %>%
  summarise(TotalIncidents = n())

#dfNumIncidentsPerDay

#Convert year to char so legend can display it correctly on line graph
dfNumIncidentsPerDay$YEAR <- as.character(dfNumIncidentsPerDay$YEAR)

#Add month name to display on output
dfNumIncidentsPerDay$MonthName <- month.name[dfNumIncidentsPerDay$MONTH]

#Plot line graph showing trends of incidents per day for each month, lines separated by year.
dfNumIncidentsPerDay %>%
  ggplot(aes(x=DAY)) +
  geom_line(aes(y=TotalIncidents, group=YEAR, color=YEAR)) +
  facet_wrap(~factor(MonthName, levels=c("January", "February", "March", "April", "May", "June", "July", "August",
                                         "September","October", "November", "December")), scales = 'free_x') +
  scale_y_continuous(limits=c(300,1000)) +
  ggtitle("Incidents Per Day By Month") +
  theme(axis.text.x = 
          element_text(size=rel(.75))) + 
  theme(axis.text.y = 
          element_text(hjust=1, vjust=.5, size=rel(.75))) + 
  xlab("Day")
```

#Q: What does the trendline look like over the entirety of the dataframe?
```{r}
#Creating Year-month-day column:
dfNumIncidentsPerDay$IncidentDate <- as.Date(with(dfNumIncidentsPerDay, paste(YEAR, MONTH, DAY,sep="-")), "%Y-%m-%d")

#Plot scatter plot of date and total number of incidents per day in 2018, and include smooth curve line
dfNumIncidentsPerDay %>%
  filter(YEAR==2018) %>%
  ggplot(aes(x=IncidentDate, y=TotalIncidents)) +
  geom_point(color="Red") +
  geom_smooth(method="loess") +
  ggtitle("2018 Total Incidents Per Day") +
  xlab("")

#Plot scatter plot of date and total number of incidents per day in 2019, and include smooth curve line
dfNumIncidentsPerDay %>%
  filter(YEAR==2019) %>%
  ggplot(aes(x=IncidentDate, y=TotalIncidents)) +
  geom_point(color="Orange") +
  geom_smooth(method="loess") +
  #facet_grid(~YEAR) + 
  ggtitle("2019 Total Incidents Per Day") +
  xlab("")
```
#Q: What are the trends of incidents per weekday for each month?
```{r}
dfNumIncidentsPerDay$weekday <- weekdays(dfNumIncidentsPerDay$IncidentDate)
#dfNumIncidentsPerDay

#Plot line graph showing trends of incidents per day for each month, lines separated by year.
dfNumIncidentsPerDay %>%
  ggplot(aes(x=weekday,y=TotalIncidents,fill=weekday))+
  geom_bar(stat='identity')+
  facet_wrap(~factor(MonthName, levels=c("January", "February", "March", "April", "May", "June", "July", "August",
                                         "September","October", "November", "December")), scales = 'free_x')+
  ggtitle("Incidents Per Month By Weekday") +
  scale_x_discrete(labels =c('M','T','W','Th','F','Sa','Su'))+ 
  theme(axis.text.y = element_text(hjust=1, vjust=.5, size=rel(.75))) + 
  xlab("Weekday")
```




#Q: What is the distribution of incidents among the boroughs?
```{r}
##Total number of incidents by Borough facet_grid with Year: 

df_test1%>%
  ggplot(aes(x=BOROUGH,y=Total,fill= BOROUGH))+
  geom_bar(stat="identity")+
  facet_grid(.~YEAR)+
  theme(axis.text.x = element_blank(),axis.ticks.x = element_blank())+
  ylab("# of pedestrians")+
  xlab("Borough")
```




#Q: Are morning or afternoon commutes more common for accidents?
```{r}
#Base data for Hour Time Series histograms
hist_Incidents <- df_test1 %>%
  ggplot(aes(x=HOUR)) +
  geom_histogram(fill='lightgreen',color="black", bins=24) +
  theme(axis.text.x = 
          element_text(angle=90, hjust=1, vjust=.5, size=rel(.85)))

#Plot histogram based on time
hist_Incidents +
  scale_x_continuous(name="Hour", breaks = 0:23) +
  scale_y_continuous(name="Total Incidents", 
                     breaks = seq(5000, 30000, 5000)) +
  xlab("Hour") +
  ylab("Total Incidents") + 
  ggtitle("Total Incidents Per Hour")+
  geom_vline(xintercept=c(9,17),color="red")

#A: Highest frequencies occur in the 4:00 and 5:00 hours
#Highest accident rate in morning commute is 8:00
#Morning commute had roughly 6,000 less total accidents than the afternoon commuting hours
```


#Q: Total Incidents per hour by borough?
```{r}
df_test1 %>%
  ggplot(aes(x=HOUR,fill=BOROUGH)) +
  geom_histogram(color='grey',bins=24) +
  facet_wrap(~BOROUGH,scales = 'free_x')+
  theme(axis.text.x = element_text(hjust=1, vjust=.5))+
  scale_x_continuous(name="Hour", breaks=seq(0,23,4))+
  geom_vline(xintercept=c(9,17),color="red")+
  theme(strip.background = element_rect(fill="lightblue", size=1, color="darkblue"))+
  xlab("Hour") +
  ylab("Total Incidents")+
   ggtitle("Total Incidents Per Hour By Borough")
hist_Incidents +
  facet_wrap(~BOROUGH) + 
  ggtitle("Total Incidents By Borough") +
  theme(axis.text.y = element_text(hjust=1, vjust=.5, size=rel(.75)))+
  scale_fill_manual(values=c("coral","yellowgreen","green","brightblue","purplepink"))
#A: All boroughs appear to follow same trend of highest accidents during afternoon commute
```


#Q: What are the different vehicle code types?
```{r}
#looking at the vehicle type code:
#unique(df_test1$VEHICLE_TYPE_CODE_1)
vehicle <-  df_test1%>%
  group_by(VEHICLE_TYPE_CODE_1)%>%
  summarise("total"=n())

vehicle2  <- arrange(vehicle,desc(total)) %>% 
  slice(-c(10))%>%
  top_n(10)


vehicle2%>%
  ggplot(aes(x=reorder(VEHICLE_TYPE_CODE_1,-total),y=total,fill=VEHICLE_TYPE_CODE_1))+
  geom_bar(stat='identity')+
  coord_flip()+
  xlab("")
```

#Q: Pedestrians, Cyclists, Motorists:
```{r}
#Total Incidents by Month 
trend_month <- df_test1%>%
  group_by(MONTH,YEAR)%>%
  summarise("Pedestrians" = sum(Pedestrian), "Cyclists"=sum(Cyclist),"Motorist"= sum(Motorist))

#trend_month

trend_month%>%
  ggplot(aes(x=MONTH))+
  geom_line(aes(y=Pedestrians,color="Pedestrians"))+
  geom_line(aes(y=Cyclists,color="Cyclists"))+
  geom_line(aes(y=Motorist,color="Motorists"))+
  facet_grid(.~YEAR)+
  ylab("total incidents")+
  scale_x_continuous(n.breaks=12)+
  labs(colour="type")
```



#Q: What day of the week Pedestrians killed?
```{r}
#Get number of incidents per day
dfNumIncidentsPerDay2 <- df_test1 %>%
  group_by(YEAR, MONTH, DAY) %>%
  summarise('ped'= sum(Pedestrian))

#dfNumIncidentsPerDay

#Convert year to char so legend can display it correctly on line graph
dfNumIncidentsPerDay2$YEAR <- as.character(dfNumIncidentsPerDay2$YEAR)

#Add month name to display on output
dfNumIncidentsPerDay2$MonthName <- month.name[dfNumIncidentsPerDay2$MONTH]

dfNumIncidentsPerDay2$IncidentDate <- as.Date(with(dfNumIncidentsPerDay2, paste(YEAR, MONTH, DAY,sep="-")), "%Y-%m-%d")

dfNumIncidentsPerDay2$weekday <- weekdays(dfNumIncidentsPerDay2$IncidentDate)
#dfNumIncidentsPerDay

#Plot line graph showing trends of incidents per day for each month, lines separated by year.
dfNumIncidentsPerDay2 %>%
  ggplot(aes(x=weekday,y=ped,fill=weekday))+
  geom_bar(stat='identity')+
  facet_wrap(~factor(MonthName, levels=c("January", "February", "March", "April", "May", "June", "July", "August",
                                         "September","October", "November", "December")), scales = 'free_x')+
  ggtitle("Incidents Per Month By Weekday for Pedestrians") +
  scale_x_discrete(labels =c('M','T','W','Th','F','Sa','Su'))+ 
  theme(axis.text.y = element_text(hjust=1, vjust=.5, size=rel(.75))) + 
  xlab("Weekday")+
  ylab("Pedestrians")
```
#Q: Pedestrians injured or killed by hour:
```{r}
#for pedestrians:

hist2 <- df_test1%>%
  group_by(HOUR)%>%
  summarise('ped'=sum(Pedestrian))

hist_Incidents2 <- hist2 %>%
  ggplot(aes(x=HOUR,y=ped)) +
  geom_bar(stat='identity',fill='lightgreen',color='black') +
  theme(axis.text.x = 
          element_text(angle=90, hjust=1, vjust=.5, size=rel(.85)))+
  xlab('HOUR')+
  ylab('Pedestrian')+
  ggtitle('Pedestrians By Hour')
hist_Incidents2
```



#Q: Pedestrians injured and killed and hour
```{r}
ph<- df_test1 %>%
  group_by(HOUR,BOROUGH)%>%
  summarise('ped'=sum(Pedestrian))
ph

ph%>%
  ggplot(aes(x=HOUR,y=ped,fill=BOROUGH)) +
  geom_bar(stat='identity')+
  facet_wrap(~BOROUGH,scales = 'free_x')+
  theme(axis.text.x = element_text(hjust=1, vjust=.5))+
  scale_x_continuous(name="Hour", breaks=seq(0,23,4))+
  geom_vline(xintercept=c(9,17),color="red")+
  theme(strip.background = element_rect(fill="lightblue", size=1, color="darkblue"))+
  xlab("Hour") +
  ylab("Pedestrians")+
   ggtitle("Total Pedestrians Per Hour By Borough")
```
#Q:Total Pedestrians Killed Per hour by Borough
```{r}
ph2<- df_test1 %>%
  group_by(HOUR,BOROUGH)%>%
  summarise('ped'=sum(pedK))
ph2

ph2%>%
  ggplot(aes(x=HOUR,y=ped,fill=BOROUGH)) +
  geom_bar(stat='identity')+
  facet_wrap(~BOROUGH,scales = 'free_x')+
  theme(axis.text.x = element_text(hjust=1, vjust=.5))+
  scale_x_continuous(name="Hour", breaks=seq(0,23,4))+
  geom_vline(xintercept=c(9,17),color="red")+
  theme(strip.background = element_rect(fill="lightblue", size=1, color="darkblue"))+
  xlab("Hour") +
  ylab("Pedestrians Killed")+
   ggtitle("Total Pedestrians Killed Per Hour By Borough")
```

#Q: What does the distribution of incidents look like on a NYC map?
```{r}
library(maps)
library(ggmap)
library(mapproj)
```

```{r}
#map of just NYC
nyc <-get_stamenmap(bbox=c(left=staten_lo_min,
                                bottom=staten_lat_min,
                                right=queens_lo_max,
                                top=bronx_lat_max), zoom=11, maptype=c("terrain"),crop=TRUE)
ggmap(nyc)
```

```{r}
#For Pedestrians Injured:
moo <- df_test1 %>%
  add_column(LEVELP = "not yet", 
             .after="ON_STREET_NAME")
#Creating Levels
moomoo <- moo %>%
  mutate(LEVELP = case_when(
                           .$pedI==2 &  .$pedK==0 & .$cylI==0 & .$cylK==0 & .$motI==0 & .$motK==0 ~ "P2",
                           .$pedI==3 &  .$pedK==0 & .$cylI==0 & .$cylK==0 & .$motI==0 & .$motK==0 ~ "P3",
                           .$pedI==4 &  .$pedK==0 & .$cylI==0 & .$cylK==0 & .$motI==0 & .$motK==0 ~ "P4",
                           .$pedI>5 &  .$pedK==0 & .$cylI==0 & .$cylK==0 & .$motI==0 & .$motK==0 ~ "P5",))
#filtering for 2018
moomoo2018 <- moomoo%>%
  filter(!is.na(moomoo$LEVELP ))%>%
  filter(YEAR==2018)

#filtering for 2019
moomoo2019 <- moomoo%>%
  filter(!is.na(moomoo$LEVELP ))%>%
  filter(YEAR==2019)

#mapping for pedestrians injured 2018
ggmap(nyc)+
  geom_point(data=moomoo2018,aes(x=LONGITUDE,y=LATITUDE,color=LEVELP,size=LEVELP))
#mapping for pedestrians injured 2019
ggmap(nyc)+
  geom_point(data=moomoo2019,aes(x=LONGITUDE,y=LATITUDE,color=LEVELP,size=LEVELP))
```

```{r}
#For Pedestrians Killed:
moo2 <- df_test1 %>%
  add_column(LEVELK = "not yet", 
             .after="ON_STREET_NAME")
#Creating Levels
moomoo2 <-moo2 %>%
  mutate(LEVELK = case_when(.$pedI==0 &  .$pedK==1 & .$cylI==0 & .$cylK==0 & .$motI==0 & .$motK==0 ~ "K1",
                            .$pedI==0 &  .$pedK==1 & .$cylI==0 & .$cylK==0 & .$motI>0 & .$motK==0 ~ "K1",
                           .$pedI==0 &  .$pedK==2 & .$cylI==0 & .$cylK==0 & .$motI>0 & .$motK==0 ~ "K2",
                           .$pedI>0 &  .$pedK==2 & .$cylI==0 & .$cylK==0 & .$motI>0 & .$motK==0 ~ "K3",))
#filtering for 2018
moomoo_2_2018 <- moomoo2%>%
  filter(!is.na(moomoo2$LEVELK ))%>%
  filter(YEAR==2018)
#filtering for 2019
moomoo_2_2019 <- moomoo2%>%
  filter(!is.na(moomoo2$LEVELK ))%>%
  filter(YEAR==2019)

#mapping for pedestrians killed 2018
ggmap(nyc)+
  geom_point(data=moomoo_2_2018,aes(x=LONGITUDE,y=LATITUDE,color=LEVELK,size=LEVELK))
#mapping for pedestrians killed 2019
ggmap(nyc)+
  geom_point(data=moomoo_2_2019,aes(x=LONGITUDE,y=LATITUDE,color=LEVELK,size=LEVELK))
```

```{r}
#Repeating the same process for for Cyclists:
co <- moo %>%
  add_column(LEVELC = "not yet", 
             .after="ON_STREET_NAME")

coco<- moo %>%
  mutate(LEVELC = case_when(
                           .$pedI==0 &  .$pedK==0 & .$cylI==2 & .$cylK==0 & .$motI==0 & .$motK==0 ~ "C2",
                           .$pedI==0 &  .$pedK==0 & .$cylI==3 & .$cylK==0 & .$motI==0 & .$motK==0 ~ "C3",
                           .$pedI==0 &  .$pedK==0 & .$cylI==0 & .$cylK==1 & .$motI==0 & .$motK==0 ~ "C4",
                           .$pedI==0 &  .$pedK==0 & .$cylI>0 & .$cylK>0 & .$motI==0 & .$motK==0 ~ "C5",
                           ))
coco2018 <- coco%>%
  filter(!is.na(coco$LEVELC))%>%
  filter(YEAR==2018)

coco2019 <- coco%>%
  filter(!is.na(coco$LEVELC))%>%
  filter(YEAR==2019)

ggmap(nyc)+
  geom_point(data=coco2018,aes(x=LONGITUDE,y=LATITUDE,color=LEVELC,size=LEVELC))

ggmap(nyc)+
  geom_point(data=coco2019,aes(x=LONGITUDE,y=LATITUDE,color=LEVELC,size=LEVELC))
```
```{r}
#Repeating the same process for motorist:
mot <- coco %>%
  add_column(LEVELM = "not yet", 
             .after="ON_STREET_NAME")

motmot <- mot %>%
  mutate(LEVELM = case_when(
                           .$pedI==0 &  .$pedK==0 & .$cylI==0 & .$cylK==0 & .$motI>5 & .$motI<=10 & .$motK==0 ~ "M2",
                           .$pedI==0 &  .$pedK==0 & .$cylI==0 & .$cylK==0 & .$motI>10 & .$motI<=15 & .$motK==0 ~ "M3",
                           .$pedI==0 &  .$pedK==0 & .$cylI==0 & .$cylK==0 & .$motI>15 &  .$motK==0 ~ "M4",
                           .$pedI==0 &  .$pedK==0 & .$cylI==0 & .$cylK==0 & .$motI==0 & .$motK==1 ~ "M5",
                           .$pedI==0 &  .$pedK==0 & .$cylI==0 & .$cylK==0 & .$motI==0 & .$motK==2 ~ "M6"))
                           
motmot2018 <- motmot%>%
  filter(!is.na(motmot$LEVELM))%>%
  filter(YEAR==2018)

motmot2019 <- motmot%>%
  filter(!is.na(motmot$LEVELM))%>%
  filter(YEAR==2019)

ggmap(nyc)+
  geom_point(data=motmot2018,aes(x=LONGITUDE,y=LATITUDE,color=LEVELM,size=LEVELM))
ggmap(nyc)+
  geom_point(data=motmot2019,aes(x=LONGITUDE,y=LATITUDE,color=LEVELM,size=LEVELM))
```

#Q: Regressions:
```{r}
r <-df_test1

r2 <-r%>%
  mutate_at("BOROUGH",str_replace,"MANHATTAN","1")%>%
  mutate_at("BOROUGH",str_replace,"QUEENS","2")%>%
  mutate_at("BOROUGH",str_replace,"BROOKLYN","3")%>%
  mutate_at("BOROUGH",str_replace,"BRONX","4")%>%
  mutate_at("BOROUGH",str_replace,"STATEN_ISLAND","5")

#setting as factor
r2$con <-factor(r2$CONTRIBUTING_FACTOR_VEHICLE_1)
is.factor(r2$con)
#setting as factor
r2$bor<-factor(r2$BOROUGH)
is.factor(r2$bor)
#running regressions to see if there's any improvements:
regression1 <- lm(Pedestrian~con,r2)
summary(regression1)
#regression2 <-lm(Pedestrian~con+bor,r2)
#summary(regression2)
#regression3 <-lm(Pedestrian~con+bor+YEAR+MONTH+DAY,r2)
#summary(regression3)
regression4 <-lm(Pedestrian~con+bor+YEAR+MONTH+DAY+HOUR+MINUTE+SECOND,r2)
summary(regression4)
#hovers around adjusted r-square of 4%...not that great
```

#SVM Prediction: Can we predict if an accident will result in an injury/death?
```{r}
library(kernlab)
library(rpart)
library(caret)
library(kernlab)
library(rpart.plot)
library(readr)
library(e1071)

#Running SVM on entire dataframe took way too long to run. Limiting dataframe to only those with at least one motorist, cyclist, or pedestrian injury or death.
dfSVMData <- df_test1 %>%
  filter(Total > 1)

#Adding bit field for pedestrians where 0 = no injury or death, and 1 = at least 1 injury or death
dfSVMData$bitPedIK <- ifelse(dfSVMData$Pedestrian >= 1, 1, 0)

#Converting bit field to factor
dfSVMData$bitPedIK <- as.factor(dfSVMData$bitPedIK)

#Limiting columns to only motorist injuries, motorist deaths, cyclist injuries, cyclist deaths, and our new bit field.
dfInjKilledData <- dfSVMData %>%
  select(motI, motK, cylI, cylK, bitPedIK)

#Dropping all possible Na's
dfInjKilledData <- dfInjKilledData %>%
  drop_na()

#Creating train list by partitioning on new bit feld at 75%
set.seed(111)
trainList <- createDataPartition(y=dfInjKilledData$bitPedIK, p=0.75, list=FALSE)

#Creating train sets and test sets based on train list.
trainSet <- dfInjKilledData[trainList, ]
testSet <- dfInjKilledData[-trainList, ]

#Validating the ratios look correct
dim(testSet)
dim(trainSet)

#Training model on train set using SVM function
trainSVMFunction <- svm(bitPedIK~., data=trainSet, type="C-classification", kernel="radial")

#Predicting pedestrian injury or death on test set based on trainSVMFunction.
svmFunctionPredict <- predict(trainSVMFunction, newdata=testSet, type="raw")

#Running confusion matrix to analyze results.
confusionMatrix(svmFunctionPredict, testSet$bitPedIK)
```
